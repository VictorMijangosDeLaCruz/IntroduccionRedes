{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf78daa",
   "metadata": {},
   "source": [
    "# Modo forward en diferenciación automática\n",
    "\n",
    "El modo forward en la diferenciación automática, como su nombre lo dice, se enfoca en encontrar la derivada de una función representada por un grafo computacional haciendo cómputos en avance. En este sentido, el grafo computacional alamcena una traza tangente, donde mientras se computan los valores de la función, se obtienen también los valores de las diferentes derivadas. Posteriormente, las derivadas de nodos superiores en el grafo se computan utilizando la regla de la cadena. \n",
    "\n",
    "De manera general, el modo forward computa una derivada en un nodo $f_i$ a partir de un nodo precedente $f_{i-1}$ por medio de la regla de la cadena de la siguiente forma:\n",
    "\n",
    "$$\\frac{\\partial f_i}{\\partial x} = \\frac{\\partial f_i}{f_{i-1}}\\frac{\\partial f_{i-1}}{\\partial x}$$\n",
    "\n",
    "De esta forma, la derivada se propaga hacia adelante, en avance, hasta obtener la derivada de las salidas de la función. En un solo paso, se pueden obtener las derivadas parciales para todas las salidas sobre una de las entradas.\n",
    "\n",
    "El modo forward se inicializa asignando un 1 a una de las variables de entrada. O de forma más general, se inicializa asignando un vector de entrada (de la misma dimensión que la entrada de la función), que generalmente es un vector base $e_j$, de tal forma que la derivada parcial se estime sobre la $j$-ésima entrada $\\frac{\\partial f}{\\partial x_j}$. Cuando se inicializa con un vector que no es base, el modo forward se interpreta como el producto entre la matriz Jacobiana $J_f$ y el vector de entrada.\n",
    "\n",
    "## Implementación de un ejemplo en modo forward\n",
    "\n",
    "Para ejemplificar el modo forward definiremos un grafo computacional que compute la función $b:\\mathbb{R}^3 \\to \\mathbb{R}$ dada como:\n",
    "\n",
    "$$b = x(y-z)^2$$\n",
    "\n",
    "En el grafo, definiremos tres nodos que nos servirán para computar la función final. Estos tres nodos son en orden los siguientes:\n",
    "\n",
    "$$a = y-z$$\n",
    "$$c = a^2$$\n",
    "$$b = xc$$\n",
    "\n",
    "Cada uno de estos nodos contiene una función más simple, cuyas derivadas son fáciles de estimar; asociaremos a cada nodo una variable con la respectiva derivada de la siguiente forma:\n",
    "\n",
    "$$a' = y'-z'$$\n",
    "$$c' = (2a)(a')$$\n",
    "$$b' = x'c+xc'$$\n",
    "\n",
    "La derivada final será la variable $b'$. Para obtenerla, usaremos el procedimiento de avance, primero asignaremos variables de la traza tangente a las variables de entrada; como señalamos estas variables serán 1 o 0, siendo 1 el valor sobre el que obtendremos las derivadas parciales. De tal forma que para estimar el vector gradiante deberemos correr el modo forward tantas veces como variables de entrada tengamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39bbb469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardMode(object):\n",
    "    def __init__(self, x1, x2, x3): \n",
    "        # Inicializa los tensores de entrada\n",
    "        self.x = x1 \n",
    "        self.y = x2 \n",
    "        self.z = x3\n",
    "\n",
    "        # Define el avance en el grafo\n",
    "        self.a = self.y-self.z\n",
    "        self.c = self.a**2\n",
    "        self.b = self.x*self.c\n",
    "        \n",
    "    def result(self):\n",
    "        # Regresa el valor de la función\n",
    "        return self.b\n",
    "        \n",
    "    def  forward(self, r=[0,0,0]):\n",
    "        \"\"\"Modo forward\"\"\"\n",
    "        # Variables para la entrada\n",
    "        dx, dy, dz = r[0], r[1], r[2]\n",
    "        # Variables para nodos\n",
    "        da = dy-dz\n",
    "        dc = 2*self.a*da\n",
    "        db = dx*self.c + self.x*dc\n",
    "        \n",
    "        return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8851ce4",
   "metadata": {},
   "source": [
    "Ahora, a partir del grafo computacional, podemos evaluar la función con valores específicos de entrada. Para obtener el vector gradiente d ela función, necesitamos obtener las derivadas parciales $\\frac{\\partial b}{\\partial x_i}$ para cada uno de los tres valores de entrada. Como puede observarse, para cada variable tomamos un vector base, lo que nos da la derivada en esa entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e87c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado de evaluar la función: 8\n",
      "Gradiente de la función: [4, 8, -8]\n"
     ]
    }
   ],
   "source": [
    "#Se asignan los valores de entrada de la función\n",
    "f = ForwardMode(2,5,3)\n",
    "\n",
    "#Se obtienen las derivadas por cada una de las variables de entrada\n",
    "dx = f.forward(r=[1,0,0])\n",
    "dy = f.forward(r=[0,1,0])\n",
    "dz = f.forward(r=[0,0,1])\n",
    "\n",
    "#Imprime valor de función\n",
    "print('Resultado de evaluar la función: {}'.format(f.result()))\n",
    "#Imprime derivadas parciales\n",
    "print('Gradiente de la función: [{}, {}, {}]'.format(dx, dy, dz))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd88610",
   "metadata": {},
   "source": [
    "Por las características del modo forward, este es útil cuando se tiene una función $f:\\mathbb{R}^n \\to \\mathbb{R}^m$ tal que $n < m$, pues en cada paso calcula las derivadas sobre todas las salidas, lo que requerirá de $n$ pasos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
